{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead09eae",
   "metadata": {},
   "source": [
    "# MongoDB to PostgreSQL Data Migration\n",
    "\n",
    "This notebook demonstrates the complete process of migrating data from MongoDB to PostgreSQL.\n",
    "\n",
    "## Overview\n",
    "- **Source Database**: MongoDB (localhost:27017, database: `tutorial`)\n",
    "- **Target Database**: PostgreSQL (localhost:5432, database: `tutorial_db`)\n",
    "- **Data Collections**: Depression Index, News Articles, Stock Data, S&P 500, Rainfall\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Create PostgreSQL Database](#create-db)\n",
    "3. [Extract and Load Standard Tables](#standard-tables)\n",
    "4. [Handle Stock Data (Normalized)](#stock-data)\n",
    "5. [Verify Migration](#verify)\n",
    "6. [Export to CSV](#export-csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255ed01",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required libraries and configure database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ecda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import os\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb81a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Configuration\n",
    "MONGO_HOST = 'localhost'\n",
    "MONGO_PORT = 27017\n",
    "MONGO_DB = 'tutorial'\n",
    "\n",
    "# PostgreSQL Configuration\n",
    "PG_USER = 'postgres'\n",
    "PG_PASSWORD = '123'  # UPDATE THIS with your password\n",
    "PG_HOST = 'localhost'\n",
    "PG_PORT = 5432\n",
    "PG_DB = 'tutorial_db'\n",
    "\n",
    "print(\"âœ“ Configuration set\")\n",
    "print(f\"MongoDB: {MONGO_HOST}:{MONGO_PORT}/{MONGO_DB}\")\n",
    "print(f\"PostgreSQL: {PG_HOST}:{PG_PORT}/{PG_DB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4248f",
   "metadata": {},
   "source": [
    "## 2. Create PostgreSQL Database\n",
    "\n",
    "Connect to PostgreSQL and create the target database if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac835627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PostgreSQL database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD,\n",
    "        database='postgres'  # Connect to default database first\n",
    "    )\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if database exists\n",
    "    cursor.execute(f\"SELECT 1 FROM pg_database WHERE datname = '{PG_DB}'\")\n",
    "    if not cursor.fetchone():\n",
    "        cursor.execute(f\"CREATE DATABASE {PG_DB}\")\n",
    "        print(f\"âœ“ Database '{PG_DB}' created\")\n",
    "    else:\n",
    "        print(f\"âœ“ Database '{PG_DB}' already exists\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connections\n",
    "mongo_client = MongoClient(MONGO_HOST, MONGO_PORT)\n",
    "mongo_db = mongo_client[MONGO_DB]\n",
    "\n",
    "pg_engine = create_engine(f'postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DB}')\n",
    "\n",
    "print(\"âœ“ Database connections established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31929e45",
   "metadata": {},
   "source": [
    "## 3. Extract and Load Standard Tables\n",
    "\n",
    "Migrate Depression Index, News Articles, S&P 500, and Rainfall data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cca73",
   "metadata": {},
   "source": [
    "### 3.1 Depression Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c21d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Depression Index data from MongoDB\n",
    "col_1 = mongo_db.Depression_index\n",
    "docs_1 = list(col_1.find({}, {\"_id\": 0, \"date\": 1, \"depression\": 1}))\n",
    "\n",
    "# Transform to DataFrame\n",
    "df_1 = pd.DataFrame(docs_1)\n",
    "df_1[\"date\"] = pd.to_datetime(df_1[\"date\"], utc=True).dt.date\n",
    "df_1.rename(columns={\"depression\": \"depression_index\"}, inplace=True)\n",
    "\n",
    "print(f\"Depression Index: {len(df_1)} records\")\n",
    "display(df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to PostgreSQL\n",
    "df_1.to_sql('depression_index', pg_engine, if_exists='replace', index=False)\n",
    "print(\"âœ“ depression_index table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09df46",
   "metadata": {},
   "source": [
    "### 3.2 News Articles (CCnews Depression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01935217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract News data from MongoDB\n",
    "col_2 = mongo_db.CCnews_Depression\n",
    "docs_2 = list(col_2.find({}, {\"_id\": 0, \"date\": 1, \"title\": 1, \"text\": 1}))\n",
    "\n",
    "# Transform to DataFrame\n",
    "df_2 = pd.DataFrame(docs_2)\n",
    "df_2[\"date\"] = pd.to_datetime(df_2[\"date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"News Articles: {len(df_2)} records\")\n",
    "display(df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52776428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to PostgreSQL\n",
    "df_2.to_sql('ccnews_depression', pg_engine, if_exists='replace', index=False)\n",
    "print(\"âœ“ ccnews_depression table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f38cd",
   "metadata": {},
   "source": [
    "### 3.3 S&P 500 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract S&P 500 data from MongoDB\n",
    "col_5 = mongo_db.SP500\n",
    "docs_5 = list(col_5.find({}, {\"_id\": 0}))\n",
    "\n",
    "# Transform to DataFrame\n",
    "df_5 = pd.DataFrame(docs_5)\n",
    "df_5[\"Date\"] = pd.to_datetime(df_5[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"S&P 500: {len(df_5)} records\")\n",
    "display(df_5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63978e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to PostgreSQL\n",
    "df_5.to_sql('sp500', pg_engine, if_exists='replace', index=False)\n",
    "print(\"âœ“ sp500 table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5984f7",
   "metadata": {},
   "source": [
    "### 3.4 Rainfall Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Rainfall data from MongoDB\n",
    "col_6 = mongo_db.Rainfall\n",
    "docs_6 = list(col_6.find({}, {\"_id\": 0}))\n",
    "\n",
    "# Transform to DataFrame\n",
    "df_6 = pd.DataFrame(docs_6)\n",
    "df_6[\"Date\"] = pd.to_datetime(df_6[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Rainfall: {len(df_6)} records\")\n",
    "display(df_6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to PostgreSQL\n",
    "df_6.to_sql('rainfall', pg_engine, if_exists='replace', index=False)\n",
    "print(\"âœ“ rainfall table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d292b8f6",
   "metadata": {},
   "source": [
    "## 4. Handle Stock Data (Normalized Format)\n",
    "\n",
    "Stock data has 2,511 columns which exceeds PostgreSQL limits. We normalize it to a long format:\n",
    "- **Original**: One row per date with 2,511 columns (Date, Open_AAPL, High_AAPL, ...)\n",
    "- **Normalized**: Multiple rows per date with 7 columns (date, ticker, open, high, low, close, volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb11b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Stock data from MongoDB\n",
    "print(\"Loading StockData from MongoDB...\")\n",
    "col_3 = mongo_db.StockData\n",
    "docs_3 = list(col_3.find({}, {\"_id\": 0}))\n",
    "\n",
    "df_3 = pd.DataFrame(docs_3)\n",
    "df_3[\"Date\"] = pd.to_datetime(df_3[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Original shape: {df_3.shape} (rows, columns)\")\n",
    "print(f\"Sample columns: {df_3.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d627cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from wide to long format\n",
    "print(\"\\nTransforming to normalized format...\")\n",
    "\n",
    "stock_cols = [col for col in df_3.columns if col != 'Date']\n",
    "records = []\n",
    "\n",
    "for idx, row in df_3.iterrows():\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Extract unique tickers\n",
    "    tickers = set()\n",
    "    for col in stock_cols:\n",
    "        parts = col.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            tickers.add(parts[-1])\n",
    "    \n",
    "    # Create one record per ticker\n",
    "    for ticker in tickers:\n",
    "        record = {\n",
    "            'date': date,\n",
    "            'ticker': ticker,\n",
    "            'open': row.get(f'Open_{ticker}'),\n",
    "            'high': row.get(f'High_{ticker}'),\n",
    "            'low': row.get(f'Low_{ticker}'),\n",
    "            'close': row.get(f'Close_{ticker}'),\n",
    "            'volume': row.get(f'Volume_{ticker}')\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"  Processed {idx + 1} rows...\")\n",
    "\n",
    "df_normalized = pd.DataFrame(records)\n",
    "print(f\"\\nNormalized shape: {df_normalized.shape}\")\n",
    "display(df_normalized.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ae75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to PostgreSQL\n",
    "print(\"\\nSending to PostgreSQL...\")\n",
    "with pg_engine.connect() as conn:\n",
    "    df_normalized.to_sql(\n",
    "        'stock_data', \n",
    "        conn, \n",
    "        if_exists='replace', \n",
    "        index=False, \n",
    "        method='multi', \n",
    "        chunksize=5000\n",
    "    )\n",
    "\n",
    "print(\"âœ“ stock_data table created\")\n",
    "print(f\"\\nTotal records: {len(df_normalized):,}\")\n",
    "print(f\"Unique tickers: {df_normalized['ticker'].nunique()}\")\n",
    "print(f\"Date range: {df_normalized['date'].min()} to {df_normalized['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e2c3b",
   "metadata": {},
   "source": [
    "## 5. Verify Migration\n",
    "\n",
    "Query PostgreSQL to verify all tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in PostgreSQL\n",
    "query = \"\"\"\n",
    "SELECT table_name \n",
    "FROM information_schema.tables \n",
    "WHERE table_schema = 'public'\n",
    "ORDER BY table_name;\n",
    "\"\"\"\n",
    "\n",
    "tables = pd.read_sql(query, pg_engine)\n",
    "print(\"Tables in PostgreSQL:\")\n",
    "display(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a73a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row counts for each table\n",
    "table_stats = []\n",
    "\n",
    "for table in ['depression_index', 'ccnews_depression', 'stock_data', 'sp500', 'rainfall']:\n",
    "    count_query = f\"SELECT COUNT(*) as count FROM {table}\"\n",
    "    result = pd.read_sql(count_query, pg_engine)\n",
    "    table_stats.append({'table': table, 'rows': result['count'][0]})\n",
    "\n",
    "df_stats = pd.DataFrame(table_stats)\n",
    "print(\"\\nTable Statistics:\")\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d05f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query: Get recent stock data for a specific ticker\n",
    "sample_query = \"\"\"\n",
    "SELECT * FROM stock_data \n",
    "WHERE ticker = 'AAPL' \n",
    "ORDER BY date DESC \n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "sample_data = pd.read_sql(sample_query, pg_engine)\n",
    "print(\"\\nSample Query - Recent AAPL stock data:\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86334fa0",
   "metadata": {},
   "source": [
    "## 6. Export to CSV Files\n",
    "\n",
    "Export all tables to CSV files for easy sharing with team members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'csv_exports'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Exporting tables to '{output_dir}' folder...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad14f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export each table\n",
    "tables_to_export = [\n",
    "    {'name': 'depression_index', 'filename': 'depression_index.csv'},\n",
    "    {'name': 'ccnews_depression', 'filename': 'ccnews_depression.csv'},\n",
    "    {'name': 'stock_data', 'filename': 'stock_data.csv'},\n",
    "    {'name': 'sp500', 'filename': 'sp500.csv'},\n",
    "    {'name': 'rainfall', 'filename': 'rainfall.csv'}\n",
    "]\n",
    "\n",
    "export_results = []\n",
    "\n",
    "for table_info in tables_to_export:\n",
    "    table_name = table_info['name']\n",
    "    filename = table_info['filename']\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    try:\n",
    "        # Read from PostgreSQL\n",
    "        df = pd.read_sql_table(table_name, pg_engine)\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "        \n",
    "        export_results.append({\n",
    "            'table': table_name,\n",
    "            'rows': len(df),\n",
    "            'file': filename,\n",
    "            'size_mb': round(file_size, 2),\n",
    "            'status': 'âœ“ Success'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        export_results.append({\n",
    "            'table': table_name,\n",
    "            'rows': 0,\n",
    "            'file': filename,\n",
    "            'size_mb': 0,\n",
    "            'status': f'âœ— Error: {e}'\n",
    "        })\n",
    "\n",
    "df_exports = pd.DataFrame(export_results)\n",
    "print(\"\\nExport Results:\")\n",
    "display(df_exports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d74a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… All files exported to: {os.path.abspath(output_dir)}\")\n",
    "print(\"\\nShare this folder with your team members!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94600cd9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Migration Complete! ðŸŽ‰\n",
    "\n",
    "**Database Information:**\n",
    "- **Source**: MongoDB (`tutorial` database)\n",
    "- **Target**: PostgreSQL (`tutorial_db` database)\n",
    "\n",
    "**Tables Created:**\n",
    "1. `depression_index` - Depression index data by date\n",
    "2. `ccnews_depression` - News articles related to depression\n",
    "3. `stock_data` - Stock market data (normalized format with 502 tickers)\n",
    "4. `sp500` - S&P 500 index data\n",
    "5. `rainfall` - Rainfall data by US state\n",
    "\n",
    "**CSV Exports:**\n",
    "- All tables exported to `csv_exports/` folder\n",
    "- Ready to share with team members\n",
    "\n",
    "### Example Queries:\n",
    "\n",
    "```sql\n",
    "-- Get Apple stock data\n",
    "SELECT * FROM stock_data WHERE ticker = 'AAPL' ORDER BY date DESC LIMIT 10;\n",
    "\n",
    "-- Compare multiple stocks\n",
    "SELECT ticker, AVG(close) as avg_close \n",
    "FROM stock_data \n",
    "WHERE ticker IN ('AAPL', 'MSFT', 'GOOGL') \n",
    "GROUP BY ticker;\n",
    "\n",
    "-- Join depression index with S&P 500\n",
    "SELECT d.date, d.depression_index, s.close_gspc \n",
    "FROM depression_index d \n",
    "JOIN sp500 s ON d.date::text = s.date;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
